# -*- coding: utf-8 -*-
"""Copy of FAQ-RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14gbNxkH1XOD1ewi4__05lGNRGACi3Bio
"""

JAI SHRI RAM



!pip install -q transformers accelerate

!pip install -q faiss-cpu rank_bm25 sentence-transformers transformers accelerate

import json
import pandas as pd

# Load the JSON file correctly
with open('/content/Ecommerce_FAQ_Chatbot_dataset.json', 'r') as f:
    data = json.load(f)

# Convert to DataFrame
faq_data = data["questions"]
df = pd.DataFrame(faq_data)

# Extract questions and answers
corpus = df['question'].tolist()
answers = df['answer'].tolist()

import nltk
from rank_bm25 import BM25Okapi

nltk.download('punkt', force=True)
nltk.download('punkt_tab', force=True)
from nltk.tokenize import word_tokenize

tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]
bm25 = BM25Okapi(tokenized_corpus)

from huggingface_hub import login

# Paste your token here (or use an environment variable securely)
login(token="")  # üîí Keep this secure!

from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize
import faiss
import numpy as np

# Load BGE-small model for RAG/FAQ
embedder = SentenceTransformer('BAAI/bge-small-en-v1.5')

# Format input for BGE (recommended prompt style)
corpus_texts = [f"Represent this sentence for retrieval: {q}" for q in corpus]

# Generate normalized embeddings
corpus_embeddings = embedder.encode(corpus_texts, convert_to_numpy=True, normalize_embeddings=True)

# Save embeddings and corpus for later use
np.savez("faq_bge_embeddings.npz", embeddings=corpus_embeddings, sentences=corpus)

# Build FAISS index with cosine similarity (inner product on normalized vectors = cosine sim)
dimension = corpus_embeddings.shape[1]
faiss_index = faiss.IndexFlatIP(dimension)
faiss_index.add(corpus_embeddings)

def search_bm25(query, top_k=3):
    tokenized_query = word_tokenize(query.lower())
    scores = bm25.get_scores(tokenized_query)
    top_indices = np.argsort(scores)[::-1][:top_k]
    return [(corpus[i], answers[i]) for i in top_indices]

def search_faiss(query, top_k=3):
    query_text = f"query: {query}"
    query_embedding = embedder.encode([query_text], convert_to_numpy=True)
    query_embedding = normalize(query_embedding, axis=1)
    D, I = faiss_index.search(query_embedding, top_k)
    return [(corpus[i], answers[i]) for i in I[0]]

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

model_id = "google/gemma-3n-e2b-it"  # Lightweight, fast Gemma model

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",  # Uses GPU if available
    torch_dtype=torch.float16  # Optimized for performance
)

llm = pipeline("text-generation", model=model, tokenizer=tokenizer)

def generate_hybrid_rag_answer_verbose(query, llm, top_k=3):
    # Step 1: Retrieve from FAISS and BM25
    faiss_results = search_faiss(query, top_k)
    bm25_results = search_bm25(query, top_k)

    # Merge + deduplicate
    combined = {}
    for q, a in faiss_results + bm25_results:
        combined[q] = a

    # Format context
    hybrid_context = "\n".join([f"Q: {q}\nA: {a}" for q, a in combined.items()])

    # LLM Prompt
    prompt = f"""You are a helpful e-commerce assistant. Use the following FAQ context to answer the user's question clearly and only once. Avoid repetition.

FAQ Context:
{hybrid_context}

User Question: {query}
Answer:"""

    # Generate response
    response = llm(prompt, max_new_tokens=200, do_sample=False)[0]["generated_text"]
    raw_answer = response.split("Answer:")[-1].strip()

    # Optional cleanup to remove redundancy
    lines = raw_answer.split('\n')
    seen = set()
    final_lines = []
    for line in lines:
        line_clean = line.strip()
        if line_clean and line_clean not in seen and not line_clean.lower().startswith("based on"):
            final_lines.append(line_clean)
            seen.add(line_clean)

    final_answer = " ".join(final_lines)

    return final_answer, faiss_results, bm25_results

model_id = "google/gemma-3n-e2b-it"

query = "Can I return a product that was bought on discount?"

print("üß† Hybrid RAG Answer (FAISS + BM25):")
answer, faiss_res, bm25_res = generate_hybrid_rag_answer_verbose(query, llm)

print("\nüîç FAISS Retrieved:")
for q, a in faiss_res:
    print(f"Q: {q}\nA: {a}\n")

print("üîç BM25 Retrieved:")
for q, a in bm25_res:
    print(f"Q: {q}\nA: {a}\n")

print("üß† Final Answer:")
print(answer)



import random
from tqdm import tqdm

def evaluate_retrievers(k=3, num_samples=30):
    correct_faiss = 0
    correct_bm25 = 0
    both_correct = 0

    indices = random.sample(range(len(corpus)), num_samples)

    for idx in tqdm(indices):
        query = corpus[idx]
        expected_answer = answers[idx]

        faiss_hits = [a for _, a in search_faiss(query, k)]
        bm25_hits = [a for _, a in search_bm25(query, k)]

        faiss_ok = expected_answer in faiss_hits
        bm25_ok = expected_answer in bm25_hits

        if faiss_ok:
            correct_faiss += 1
        if bm25_ok:
            correct_bm25 += 1
        if faiss_ok and bm25_ok:
            both_correct += 1

    print(f"üìä FAISS Recall@{k}: {correct_faiss}/{num_samples} = {correct_faiss / num_samples:.2f}")
    print(f"üìä BM25 Recall@{k}: {correct_bm25}/{num_samples} = {correct_bm25 / num_samples:.2f}")
    print(f"üîÅ Both retrieved correctly: {both_correct}/{num_samples} = {both_correct / num_samples:.2f}")

evaluate_retrievers(k=3, num_samples=30)

!pip install rouge-score nltk

import random
from tqdm import tqdm
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer
import pandas as pd

def evaluate_rag_pipeline_with_llm(llm, k=3, num_samples=30):
    results = []
    indices = random.sample(range(len(corpus)), num_samples)

    bleu_scores = []
    rouge_1_scores = []
    rouge_l_scores = []

    for idx in tqdm(indices):
        query = corpus[idx]
        expected_answer = answers[idx]

        # Get generated RAG answer
        generated_answer, faiss_res, bm25_res = generate_hybrid_rag_answer_verbose(query, llm, top_k=k)

        # BLEU Score
        reference = [expected_answer.split()]
        hypothesis = generated_answer.split()
        bleu = sentence_bleu(reference, hypothesis, smoothing_function=SmoothingFunction().method1)
        bleu_scores.append(bleu)

        # ROUGE Score
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
        scores = scorer.score(expected_answer, generated_answer)
        rouge_1_scores.append(scores['rouge1'].fmeasure)
        rouge_l_scores.append(scores['rougeL'].fmeasure)

        results.append({
            "query": query,
            "expected_answer": expected_answer,
            "generated_answer": generated_answer,
            "bleu": bleu,
            "rouge1": scores['rouge1'].fmeasure,
            "rougeL": scores['rougeL'].fmeasure
        })

    # Averages
    avg_bleu = sum(bleu_scores) / len(bleu_scores)
    avg_rouge1 = sum(rouge_1_scores) / len(rouge_1_scores)
    avg_rougeL = sum(rouge_l_scores) / len(rouge_l_scores)

    print(f"\nüìä Average BLEU: {avg_bleu:.4f}")
    print(f"üìä Average ROUGE-1 F1: {avg_rouge1:.4f}")
    print(f"üìä Average ROUGE-L F1: {avg_rougeL:.4f}")

    return pd.DataFrame(results)

df_eval = evaluate_rag_pipeline_with_llm(llm, k=3, num_samples=30)
df_eval.head()
